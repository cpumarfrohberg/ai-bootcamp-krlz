{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5f538f3-ab9b-40ba-adf0-6f1028b8cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from minsearch import Index\n",
    "\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "from typing import List, Any, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1f6839",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDS_PATH = Path.cwd() / '..' / '..' / '.env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64022044-47de-4794-9d74-9144f8328a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(CREDS_PATH)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1842d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dedd071",
   "metadata": {},
   "source": [
    "### Q4: How many records are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06efd97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = ' https://github.com/DataTalksClub/datatalksclub.github.io/tree/main/_podcast'\n",
    "DATA = 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/contents/_podcast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "400b4171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "Content type: application/json; charset=utf-8\n",
      "Response length: 212835\n",
      "First 200 characters: [{\"name\":\"_s12e08.md\",\"path\":\"_podcast/_s12e08.md\",\"sha\":\"713ef42e7cc080cbb8c6e1ae4978fa0b5a4304b3\",\"size\":56384,\"url\":\"https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/contents/_pod\n",
      "=== GITHUB API METADATA ===\n",
      "Total files found: 185\n",
      "\n",
      "File types and names:\n",
      "1. _s12e08.md (type: file, size: 56384 bytes)\n",
      "2. _template.md (type: file, size: 284 bytes)\n",
      "3. s01e01-roles.md (type: file, size: 430 bytes)\n",
      "4. s01e02-processes.md (type: file, size: 494 bytes)\n",
      "5. s01e03-building-ds-team.md (type: file, size: 52722 bytes)\n",
      "6. s01e04-standing-out-as-a-data-scientist.md (type: file, size: 63713 bytes)\n",
      "7. s01e05-mentoring.md (type: file, size: 375 bytes)\n",
      "8. s02e01-writing.md (type: file, size: 505 bytes)\n",
      "9. s02e02-developer-advocacy.md (type: file, size: 54972 bytes)\n",
      "10. s02e03-open-source.md (type: file, size: 654 bytes)\n",
      "1. _s12e08.md (type: file, size: 56384 bytes)\n",
      "2. _template.md (type: file, size: 284 bytes)\n",
      "3. s01e01-roles.md (type: file, size: 430 bytes)\n",
      "4. s01e02-processes.md (type: file, size: 494 bytes)\n",
      "5. s01e03-building-ds-team.md (type: file, size: 52722 bytes)\n",
      "6. s01e04-standing-out-as-a-data-scientist.md (type: file, size: 63713 bytes)\n",
      "7. s01e05-mentoring.md (type: file, size: 375 bytes)\n",
      "8. s02e01-writing.md (type: file, size: 505 bytes)\n",
      "9. s02e02-developer-advocacy.md (type: file, size: 54972 bytes)\n",
      "10. s02e03-open-source.md (type: file, size: 654 bytes)\n",
      "... and 175 more files\n",
      "\n",
      "First file details:\n",
      "Name: _s12e08.md\n",
      "Type: file\n",
      "Size: 56384 bytes\n",
      "Download URL: https://raw.githubusercontent.com/DataTalksClub/datatalksclub.github.io/main/_podcast/_s12e08.md\n",
      "\n",
      "Metadata keys: ['name', 'path', 'sha', 'size', 'url', 'html_url', 'git_url', 'download_url', 'type', '_links']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': '_s12e08.md',\n",
       "  'path': '_podcast/_s12e08.md',\n",
       "  'sha': '713ef42e7cc080cbb8c6e1ae4978fa0b5a4304b3',\n",
       "  'size': 56384,\n",
       "  'url': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/contents/_podcast/_s12e08.md?ref=main',\n",
       "  'html_url': 'https://github.com/DataTalksClub/datatalksclub.github.io/blob/main/_podcast/_s12e08.md',\n",
       "  'git_url': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/git/blobs/713ef42e7cc080cbb8c6e1ae4978fa0b5a4304b3',\n",
       "  'download_url': 'https://raw.githubusercontent.com/DataTalksClub/datatalksclub.github.io/main/_podcast/_s12e08.md',\n",
       "  'type': 'file',\n",
       "  '_links': {'self': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/contents/_podcast/_s12e08.md?ref=main',\n",
       "   'git': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/git/blobs/713ef42e7cc080cbb8c6e1ae4978fa0b5a4304b3',\n",
       "   'html': 'https://github.com/DataTalksClub/datatalksclub.github.io/blob/main/_podcast/_s12e08.md'}},\n",
       " {'name': '_template.md',\n",
       "  'path': '_podcast/_template.md',\n",
       "  'sha': 'b9ce28148e2ab58a7bff3972c81469709ab9c190',\n",
       "  'size': 284,\n",
       "  'url': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/contents/_podcast/_template.md?ref=main',\n",
       "  'html_url': 'https://github.com/DataTalksClub/datatalksclub.github.io/blob/main/_podcast/_template.md',\n",
       "  'git_url': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/git/blobs/b9ce28148e2ab58a7bff3972c81469709ab9c190',\n",
       "  'download_url': 'https://raw.githubusercontent.com/DataTalksClub/datatalksclub.github.io/main/_podcast/_template.md',\n",
       "  'type': 'file',\n",
       "  '_links': {'self': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/contents/_podcast/_template.md?ref=main',\n",
       "   'git': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/git/blobs/b9ce28148e2ab58a7bff3972c81469709ab9c190',\n",
       "   'html': 'https://github.com/DataTalksClub/datatalksclub.github.io/blob/main/_podcast/_template.md'}},\n",
       " {'name': 's01e01-roles.md',\n",
       "  'path': '_podcast/s01e01-roles.md',\n",
       "  'sha': 'd3738cd2405f7de006ec2c0fc14b90ddb77d3b3a',\n",
       "  'size': 430,\n",
       "  'url': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/contents/_podcast/s01e01-roles.md?ref=main',\n",
       "  'html_url': 'https://github.com/DataTalksClub/datatalksclub.github.io/blob/main/_podcast/s01e01-roles.md',\n",
       "  'git_url': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/git/blobs/d3738cd2405f7de006ec2c0fc14b90ddb77d3b3a',\n",
       "  'download_url': 'https://raw.githubusercontent.com/DataTalksClub/datatalksclub.github.io/main/_podcast/s01e01-roles.md',\n",
       "  'type': 'file',\n",
       "  '_links': {'self': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/contents/_podcast/s01e01-roles.md?ref=main',\n",
       "   'git': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/git/blobs/d3738cd2405f7de006ec2c0fc14b90ddb77d3b3a',\n",
       "   'html': 'https://github.com/DataTalksClub/datatalksclub.github.io/blob/main/_podcast/s01e01-roles.md'}},\n",
       " {'name': 's01e02-processes.md',\n",
       "  'path': '_podcast/s01e02-processes.md',\n",
       "  'sha': '3269ddfcb8bc3348f029a22778f785396e121270',\n",
       "  'size': 494,\n",
       "  'url': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/contents/_podcast/s01e02-processes.md?ref=main',\n",
       "  'html_url': 'https://github.com/DataTalksClub/datatalksclub.github.io/blob/main/_podcast/s01e02-processes.md',\n",
       "  'git_url': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/git/blobs/3269ddfcb8bc3348f029a22778f785396e121270',\n",
       "  'download_url': 'https://raw.githubusercontent.com/DataTalksClub/datatalksclub.github.io/main/_podcast/s01e02-processes.md',\n",
       "  'type': 'file',\n",
       "  '_links': {'self': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/contents/_podcast/s01e02-processes.md?ref=main',\n",
       "   'git': 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/git/blobs/3269ddfcb8bc3348f029a22778f785396e121270',\n",
       "   'html': 'https://github.com/DataTalksClub/datatalksclub.github.io/blob/main/_podcast/s01e02-processes.md'}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's see what files are available in the _podcast directory\n",
    "docs_raw = requests.get(DATA)\n",
    "\n",
    "# Debug: Check the response before trying to parse JSON\n",
    "print(f\"Status code: {docs_raw.status_code}\")\n",
    "print(f\"Content type: {docs_raw.headers.get('content-type', 'Unknown')}\")\n",
    "print(f\"Response length: {len(docs_raw.text)}\")\n",
    "print(f\"First 200 characters: {docs_raw.text[:200]}\")\n",
    "\n",
    "if docs_raw.status_code == 200:\n",
    "    try:\n",
    "        files_list = docs_raw.json()\n",
    "        print(\"=== GITHUB API METADATA ===\")\n",
    "        print(f\"Total files found: {len(files_list)}\")\n",
    "        print(\"\\nFile types and names:\")\n",
    "        for i, file_info in enumerate(files_list[:10]):  # Show first 10 files\n",
    "            print(f\"{i+1}. {file_info['name']} (type: {file_info['type']}, size: {file_info['size']} bytes)\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decode error: {e}\")\n",
    "        print(\"Response is not valid JSON. This might be due to:\")\n",
    "        print(\"1. Rate limiting (GitHub API)\")\n",
    "        print(\"2. Authentication issues\")\n",
    "        print(\"3. Invalid endpoint\")\n",
    "else:\n",
    "    print(f\"HTTP error: {docs_raw.status_code}\")\n",
    "    print(\"Response:\", docs_raw.text[:500])\n",
    "for i, file_info in enumerate(files_list[:10]):  # Show first 10 files\n",
    "    print(f\"{i+1}. {file_info['name']} (type: {file_info['type']}, size: {file_info['size']} bytes)\")\n",
    "\n",
    "if len(files_list) > 10:\n",
    "    print(f\"... and {len(files_list) - 10} more files\")\n",
    "\n",
    "print(f\"\\nFirst file details:\")\n",
    "print(f\"Name: {files_list[0]['name']}\")\n",
    "print(f\"Type: {files_list[0]['type']}\")\n",
    "print(f\"Size: {files_list[0]['size']} bytes\")\n",
    "print(f\"Download URL: {files_list[0]['download_url']}\")\n",
    "\n",
    "# Show the structure of the metadata\n",
    "print(f\"\\nMetadata keys: {list(files_list[0].keys())}\")\n",
    "files_list[0:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c27380e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRANSCRIPT PROCESSING ===\n",
      "All file extensions found:\n",
      "Extensions: ['md']\n",
      "\n",
      "First 10 filenames:\n",
      "1. _s12e08.md\n",
      "2. _template.md\n",
      "3. s01e01-roles.md\n",
      "4. s01e02-processes.md\n",
      "5. s01e03-building-ds-team.md\n",
      "6. s01e04-standing-out-as-a-data-scientist.md\n",
      "7. s01e05-mentoring.md\n",
      "8. s02e01-writing.md\n",
      "9. s02e02-developer-advocacy.md\n",
      "10. s02e03-open-source.md\n",
      "\n",
      "Found 185 Markdown files\n",
      "Using file: _s12e08.md\n",
      "Loaded markdown content (55835 characters)\n",
      "\n",
      "First 10 lines of the markdown file:\n",
      "1: ---\n",
      "2: episode: 8\n",
      "3: guests:\n",
      "4: - jekaterinakokatjuhha\n",
      "5: ids:\n",
      "6:   anchor: The-Journey-of-a-Data-Generalist-From-Bioinformatics-to-Freelancing---Jekaterina-Kokatjuhha-e1upvim\n",
      "7:   youtube: FRi0SUtxdMw\n",
      "8: image: images/podcast/s12e08-journey-of-data-generalist-from-bioinformatics-to-freelancing.jpg\n",
      "9: links:\n",
      "10:   anchor: https://anchor.fm/datatalksclub/episodes/The-Journey-of-a-Data-Generalist-From-Bioinformatics-to-Freelancing---Jekaterina-Kokatjuhha-e1upvim\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRANSCRIPT PROCESSING ===\")\n",
    "\n",
    "print(\"All file extensions found:\")\n",
    "extensions = set()\n",
    "for f in files_list:\n",
    "    if '.' in f['name']:\n",
    "        ext = f['name'].split('.')[-1]\n",
    "        extensions.add(ext)\n",
    "    else:\n",
    "        extensions.add('no extension')\n",
    "print(f\"Extensions: {sorted(extensions)}\")\n",
    "\n",
    "# Let's also see some actual filenames\n",
    "print(f\"\\nFirst 10 filenames:\")\n",
    "for i, f in enumerate(files_list[:10]):\n",
    "    print(f\"{i+1}. {f['name']}\")\n",
    "\n",
    "md_files = [f for f in files_list if f['name'].endswith('.md')]\n",
    "print(f\"\\nFound {len(md_files)} Markdown files\")\n",
    "\n",
    "# For now, let's use the first Markdown file as an example\n",
    "if md_files:\n",
    "    first_file = md_files[0]\n",
    "    print(f\"Using file: {first_file['name']}\")\n",
    "    \n",
    "    # Get the actual content of the file (Markdown is text, not JSON)\n",
    "    content_response = requests.get(first_file['download_url'])\n",
    "    markdown_content = content_response.text\n",
    "    print(f\"Loaded markdown content ({len(markdown_content)} characters)\")\n",
    "    \n",
    "    # Show first few lines to understand the structure\n",
    "    lines = markdown_content.split('\\n')\n",
    "    print(f\"\\nFirst 10 lines of the markdown file:\")\n",
    "    for i, line in enumerate(lines[:10]):\n",
    "        print(f\"{i+1}: {line}\")\n",
    "    \n",
    "    # For now, we'll work with the raw markdown content\n",
    "    docs_response = markdown_content\n",
    "else:\n",
    "    print(\"No Markdown files found\")\n",
    "    docs_response = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54e5efee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed 128 transcript entries\n",
      "First few entries:\n",
      "1. 71s: '...\n",
      "2. 112s: '...\n",
      "3. 122s: '...\n",
      "4. 135s: '...\n",
      "5. 135s: '...\n"
     ]
    }
   ],
   "source": [
    "# Create a simple class to represent transcript entries\n",
    "class TranscriptEntry:\n",
    "    def __init__(self, start, text):\n",
    "        self.start = start\n",
    "        self.text = text\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"TranscriptEntry(start={self.start}s, text='{self.text[:50]}{'...' if len(self.text) > 50 else ''}')\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "# Try to parse timestamps and text from the markdown\n",
    "transcript_entries = []\n",
    "timestamp_pattern = r'(\\d{1,2}:\\d{2}(?::\\d{2})?)'  # Matches MM:SS or HH:MM:SS\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    \n",
    "    # Look for timestamp patterns\n",
    "    match = re.search(timestamp_pattern, line)\n",
    "    if match:\n",
    "        timestamp_str = match.group(1)\n",
    "        # Convert timestamp to seconds\n",
    "        parts = timestamp_str.split(':')\n",
    "        if len(parts) == 2:  # MM:SS\n",
    "            seconds = int(parts[0]) * 60 + int(parts[1])\n",
    "        elif len(parts) == 3:  # HH:MM:SS\n",
    "            seconds = int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Extract text after timestamp\n",
    "        text = line[match.end():].strip()\n",
    "        if text:\n",
    "            transcript_entries.append(TranscriptEntry(seconds, text))\n",
    "\n",
    "print(f\"\\nParsed {len(transcript_entries)} transcript entries\")\n",
    "if transcript_entries:\n",
    "    print(\"First few entries:\")\n",
    "    for i, entry in enumerate(transcript_entries[:5]):\n",
    "        print(f\"{i+1}. {entry.start}s: {entry.text[:50]}...\")\n",
    "\n",
    "# Update docs_response to be the parsed transcript entries\n",
    "docs_response = transcript_entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af1cdbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 128 records'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'There are {len(docs_response)} records'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee816242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TranscriptEntry(start=1282s, text=''')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_response[43]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a59880d",
   "metadata": {},
   "source": [
    "### Q5: How many chunks do you have in the result?\n",
    "* chunk size 30 \n",
    "* overlap 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2558268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timestamp(seconds: float) -> str:\n",
    "    \"\"\"Convert seconds to H:MM:SS if > 1 hour, else M:SS\"\"\"\n",
    "    total_seconds = int(seconds)\n",
    "    hours, remainder = divmod(total_seconds, 3600)\n",
    "    minutes, secs = divmod(remainder, 60)\n",
    "\n",
    "    if hours > 0:\n",
    "        return f\"{hours}:{minutes:02}:{secs:02}\"\n",
    "    else:\n",
    "        return f\"{minutes}:{secs:02}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb374fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subtitles(transcript: List[Any]) -> str:\n",
    "    lines = []\n",
    "\n",
    "    for entry in transcript:\n",
    "        ts = format_timestamp(entry.start)\n",
    "        text = entry.text.replace('\\n', ' ')\n",
    "        lines.append(ts + ' ' + text)\n",
    "\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abb5b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq: List[Any], size: int, step: int) -> List[List[Any]]:\n",
    "    \"\"\"Create overlapping chunks using sliding window approach.\"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "\n",
    "    for i in range(0, n, step):\n",
    "        batch = seq[i:i+size]\n",
    "        result.append(batch)\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2fc2a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_lines(transcript: List[Any]) -> str:\n",
    "    \"\"\"Join transcript entries into continuous text.\"\"\"\n",
    "    lines = []\n",
    "\n",
    "    for entry in transcript:\n",
    "        text = entry.text.replace('\\n', ' ')\n",
    "        lines.append(text)\n",
    "\n",
    "    return ' '.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32132939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chunk(chunk: List[Any]) -> Dict[str, str]:\n",
    "    \"\"\"Format a chunk with start/end timestamps and text.\"\"\"\n",
    "    time_start = format_timestamp(chunk[0].start)\n",
    "    time_end = format_timestamp(chunk[-1].start)\n",
    "    text = join_lines(chunk)\n",
    "\n",
    "    return {\n",
    "        'start': time_start,\n",
    "        'end': time_end,\n",
    "        'text': text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02c7bc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are a total of 8 chunks'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = []\n",
    "\n",
    "for chunk in sliding_window(docs_response, 30, 15):  #step = chunk_size - overlap\n",
    "    processed = format_chunk(chunk)\n",
    "    chunks.append(processed)\n",
    "\n",
    "f'There are a total of {len(chunks)} chunks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0d247",
   "metadata": {},
   "source": [
    "#### Q6: What's the first episode in the results for \"how do I make money with AI?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ca3eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = 'how do I make money with AI?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f2ab68",
   "metadata": {},
   "source": [
    "* Search (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22cf6f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x11de2f790>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "    text_fields=[\"short\", \"title\"],\n",
    ")\n",
    "\n",
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "286a2668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    return index.search(\n",
    "        query=query,\n",
    "        num_results=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c29951",
   "metadata": {},
   "source": [
    "* Build prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "519d781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You're an assistant that helps with the documentation.\n",
    "Answer the QUESTION based on the CONTEXT from the search engine of our documentation.\n",
    "\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "When answering the question, provide the reference to the file with the source.\n",
    "Use the filename field for that. The repo url is: https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/contents/_podcast\n",
    "Include code examples when relevant. \n",
    "If the question is discussed in multiple documents, cite all of them.\n",
    "\n",
    "Don't use markdown or any formatting in the output.\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "462ff5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, search_results):\n",
    "    context = json.dumps(search_results)\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    ).strip()\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb76aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(user_prompt, instructions=None, model=\"gpt-4o-mini\"):\n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "165bf9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    response = llm(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00dc1816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Making money with AI can be approached in various ways depending on your skills, resources, and market demand. Here are some ideas:\\n\\n1. **Develop AI Applications**: If you have coding skills, consider creating applications that leverage AI for specific industries (healthcare, finance, education, etc.).\\n\\n2. **Freelance AI Solutions**: Offer freelance services in machine learning, data analysis, or natural language processing for businesses that need AI expertise.\\n\\n3. **AI Consulting**: Help companies integrate AI into their processes, providing your expertise on strategy and implementation.\\n\\n4. **Online Courses and Tutorials**: Create and sell online courses teaching AI concepts and applications. Platforms like Udemy or Coursera can be used.\\n\\n5. **Data Annotation Services**: Offer services to label or annotate data for AI training. Many companies require large datasets to train their models.\\n\\n6. **Affiliate Marketing with AI Tools**: Promote AI-driven tools or platforms through affiliate marketing, earning commissions on sales.\\n\\n7. **Content Creation**: Use AI tools to generate content (blog posts, videos, etc.) and monetize through ads or sponsorships.\\n\\n8. **AI in E-commerce**: Use AI to analyze customer behavior and personalize marketing strategies in your own e-commerce business.\\n\\n9. **Sell AI Models**: If you develop a unique AI model, consider selling it to businesses or licensing it for use in specific applications.\\n\\n10. **Invest in AI Startups**: If you have capital, consider investing in AI startups that show promise.\\n\\n11. **Develop Chatbots**: Create chatbots for businesses to enhance customer service and support.\\n\\n12. **Integrate AI for Efficiency**: If you run a business, implement AI solutions to streamline operations, which can increase profitability.\\n\\n13. **Participate in Competitions**: Join AI hackathons or competitions that offer cash prizes for innovative solutions. \\n\\nEach option has its own requirements, so consider what aligns best with your skills and interests.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(QUESTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e2fdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
