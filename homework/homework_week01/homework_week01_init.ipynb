{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f538f3-ab9b-40ba-adf0-6f1028b8cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "from typing import List, Any, Dict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1f6839",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDS_PATH = Path.cwd() / '..' / '..' / '.env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64022044-47de-4794-9d74-9144f8328a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(CREDS_PATH)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1842d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dedd071",
   "metadata": {},
   "source": [
    "### Q4: How many records are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06efd97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = ' https://github.com/DataTalksClub/datatalksclub.github.io/tree/main/_podcast'\n",
    "DATA = 'https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/contents/_podcast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e5efee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PARSING MARKDOWN TO TRANSCRIPT FORMAT ===\n",
      "Total lines in markdown: 1042\n",
      "\n",
      "First 20 lines:\n",
      " 1: ---\n",
      " 2: episode: 8\n",
      " 3: guests:\n",
      " 4: - jekaterinakokatjuhha\n",
      " 5: ids:\n",
      " 6:   anchor: The-Journey-of-a-Data-Generalist-From-Bioinformatics-to-Freelancing---Jekaterina-Kokatjuhha-e1upvim\n",
      " 7:   youtube: FRi0SUtxdMw\n",
      " 8: image: images/podcast/s12e08-journey-of-data-generalist-from-bioinformatics-to-freelancing.jpg\n",
      " 9: links:\n",
      "10:   anchor: https://anchor.fm/datatalksclub/episodes/The-Journey-of-a-Data-Generalist-From-Bioinformatics-to-Freelancing---Jekaterina-Kokatjuhha-e1upvim\n",
      "11:   apple: https://podcasts.apple.com/us/podcast/the-journey-of-a-data-generalist-from/id1541710331?i=1000599125044\n",
      "12:   spotify: https://open.spotify.com/episode/5fB185hGlGYQmdk0kbIsPv?si=YtnsaYNzTc-fl7emZ2IjEA\n",
      "13:   youtube: https://www.youtube.com/watch?v=FRi0SUtxdMw\n",
      "14: season: 12\n",
      "15: short: 'The Journey of a Data Generalist: From Bioinformatics to Freelancing'\n",
      "16: title: 'The Journey of a Data Generalist: From Bioinformatics to Freelancing'\n",
      "17: transcript:\n",
      "18: - line: This week we'll talk about being a data generalist. We'll discuss going from\n",
      "19:     bioinformatics to freelancing. We have a special guest today, Katya. As a freelancer\n",
      "20:     Katya is helping companies bridge the gap between business and data by building\n",
      "\n",
      "=== LOOKING FOR TRANSCRIPT SECTION ===\n",
      "Found transcript section at line 17: transcript:\n",
      "\n",
      "Lines around transcript section:\n",
      "15: short: 'The Journey of a Data Generalist: From Bioinformatics to Freelancing'\n",
      "16: title: 'The Journey of a Data Generalist: From Bioinformatics to Freelancing'\n",
      "17: transcript:\n",
      "18: - line: This week we'll talk about being a data generalist. We'll discuss going from\n",
      "19:     bioinformatics to freelancing. We have a special guest today, Katya. As a freelancer\n",
      "20:     Katya is helping companies bridge the gap between business and data by building\n",
      "21:     actionable analytics and coaching the teams. She has a lot of broad experience\n",
      "22:     in startups, entrepreneurship and scale-ups. Katya was head of analytics at Gitti,\n",
      "23:     a beauty brand. She tried to start her own fintech business with Entrepreneur\n",
      "24:     First and she worked as a data scientist at Zalando. Welcome to the show. It's\n",
      "25:     a pleasure to have you here.\n",
      "26:   sec: 71\n",
      "27:   time: '1:11'\n",
      "28:   who: Alexey\n",
      "29: - line: Yes, thank you so much for the invitation. It was really nice to catch up,\n",
      "30:     actually. I think we've known each other for some time. I'm really happy to be\n",
      "31:     here.\n",
      "32:   sec: 112\n",
      "33:   time: '1:52'\n",
      "34:   who: Jekaterina\n",
      "35: - header: Jekaterinaâ€™s background\n",
      "36: - line: I tried to invite you multiple times. Finally, we managed to do this. [chuckles]\n",
      "\n",
      "Parsed 128 transcript entries\n",
      "First few entries:\n",
      "1. 71s: '...\n",
      "2. 112s: '...\n",
      "3. 122s: '...\n",
      "4. 135s: '...\n",
      "5. 135s: '...\n"
     ]
    }
   ],
   "source": [
    "class TranscriptEntry:\n",
    "    def __init__(self, start, text):\n",
    "        self.start = start\n",
    "        self.text = text\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"TranscriptEntry(start={self.start}s, text='{self.text[:50]}{'...' if len(self.text) > 50 else ''}')\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "# Try to parse timestamps and text from the markdown\n",
    "transcript_entries = []\n",
    "timestamp_pattern = r'(\\d{1,2}:\\d{2}(?::\\d{2})?)'  # Matches MM:SS or HH:MM:SS\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    \n",
    "    # Look for timestamp patterns\n",
    "    match = re.search(timestamp_pattern, line)\n",
    "    if match:\n",
    "        timestamp_str = match.group(1)\n",
    "        parts = timestamp_str.split(':')\n",
    "        if len(parts) == 2:  # MM:SS\n",
    "            seconds = int(parts[0]) * 60 + int(parts[1])\n",
    "        elif len(parts) == 3:  # HH:MM:SS\n",
    "            seconds = int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Extract text after timestamp\n",
    "        text = line[match.end():].strip()\n",
    "        if text:\n",
    "            transcript_entries.append(TranscriptEntry(seconds, text))\n",
    "\n",
    "print(f\"\\nParsed {len(transcript_entries)} transcript entries\")\n",
    "if transcript_entries:\n",
    "    print(\"First few entries:\")\n",
    "    for i, entry in enumerate(transcript_entries[:5]):\n",
    "        print(f\"{i+1}. {entry.start}s: {entry.text[:50]}...\")\n",
    "\n",
    "# Update docs_response to be the parsed transcript entries\n",
    "docs_response = transcript_entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af1cdbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 128 records'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'There are {len(docs_response)} records'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee816242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TranscriptEntry(start=1282s, text=''')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_response[43]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a59880d",
   "metadata": {},
   "source": [
    "### Q5: How many chunks do you have in the result?\n",
    "* chunk size 30 \n",
    "* overlap 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2558268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timestamp(seconds: float) -> str:\n",
    "    \"\"\"Convert seconds to H:MM:SS if > 1 hour, else M:SS\"\"\"\n",
    "    total_seconds = int(seconds)\n",
    "    hours, remainder = divmod(total_seconds, 3600)\n",
    "    minutes, secs = divmod(remainder, 60)\n",
    "\n",
    "    if hours > 0:\n",
    "        return f\"{hours}:{minutes:02}:{secs:02}\"\n",
    "    else:\n",
    "        return f\"{minutes}:{secs:02}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb374fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subtitles(transcript: List[Any]) -> str:\n",
    "    lines = []\n",
    "\n",
    "    for entry in transcript:\n",
    "        ts = format_timestamp(entry.start)\n",
    "        text = entry.text.replace('\\n', ' ')\n",
    "        lines.append(ts + ' ' + text)\n",
    "\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abb5b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq: List[Any], size: int, step: int) -> List[List[Any]]:\n",
    "    \"\"\"Create overlapping chunks using sliding window approach.\"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "\n",
    "    for i in range(0, n, step):\n",
    "        batch = seq[i:i+size]\n",
    "        result.append(batch)\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2fc2a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_lines(transcript: List[Any]) -> str:\n",
    "    \"\"\"Join transcript entries into continuous text.\"\"\"\n",
    "    lines = []\n",
    "\n",
    "    for entry in transcript:\n",
    "        text = entry.text.replace('\\n', ' ')\n",
    "        lines.append(text)\n",
    "\n",
    "    return ' '.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32132939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chunk(chunk: List[Any]) -> Dict[str, str]:\n",
    "    \"\"\"Format a chunk with start/end timestamps and text.\"\"\"\n",
    "    time_start = format_timestamp(chunk[0].start)\n",
    "    time_end = format_timestamp(chunk[-1].start)\n",
    "    text = join_lines(chunk)\n",
    "\n",
    "    return {\n",
    "        'start': time_start,\n",
    "        'end': time_end,\n",
    "        'text': text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02c7bc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are a total of 8 chunks'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = []\n",
    "\n",
    "for chunk in sliding_window(docs_response, 30, 15):  #step = chunk_size - overlap\n",
    "    processed = format_chunk(chunk)\n",
    "    chunks.append(processed)\n",
    "\n",
    "f'There are a total of {len(chunks)} chunks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0d247",
   "metadata": {},
   "source": [
    "#### Q6: What's the first episode in the results for \"how do I make money with AI?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf6f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
